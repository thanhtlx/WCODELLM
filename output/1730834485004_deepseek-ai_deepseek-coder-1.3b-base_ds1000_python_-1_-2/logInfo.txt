Prompt: Problem:
I have been trying to get the result of a lognormal distribution using Scipy. I already have the Mu and Sigma, so I don't need to do any other prep work. If I need to be more specific (and I am trying to be with my limited knowledge of stats), I would say that I am looking for the cumulative function (cdf under Scipy). The problem is that I can't figure out how to do this with just the mean and standard deviation on a scale of 0-1 (ie the answer returned should be something from 0-1). I'm also not sure which method from dist, I should be using to get the answer. I've tried reading the documentation and looking through SO, but the relevant questions (like this and this) didn't seem to provide the answers I was looking for.
Here is a code sample of what I am working with. Thanks. Here mu and stddev stands for mu and sigma in probability density function of lognorm.
from scipy.stats import lognorm
stddev = 0.859455801705594
mu = 0.418749176686875
total = 37
dist = lognorm.cdf(total,mu,stddev)
UPDATE:
So after a bit of work and a little research, I got a little further. But I still am getting the wrong answer. The new code is below. According to R and Excel, the result should be .7434, but that's clearly not what is happening. Is there a logic flaw I am missing?
stddev = 2.0785
mu = 1.744
x = 25
dist = lognorm([mu],loc=stddev)
dist.cdf(x)  # yields=0.96374596, expected=0.7434
A:
<code>
import numpy as np
from scipy import stats
stddev = 2.0785
mu = 1.744
x = 25
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>

Problem: Problem:
I have been trying to get the result of a lognormal distribution using Scipy. I already have the Mu and Sigma, so I don't need to do any other prep work. If I need to be more specific (and I am trying to be with my limited knowledge of stats), I would say that I am looking for the cumulative function (cdf under Scipy). The problem is that I can't figure out how to do this with just the mean and standard deviation on a scale of 0-1 (ie the answer returned should be something from 0-1). I'm also not sure which method from dist, I should be using to get the answer. I've tried reading the documentation and looking through SO, but the relevant questions (like this and this) didn't seem to provide the answers I was looking for.
Here is a code sample of what I am working with. Thanks. Here mu and stddev stands for mu and sigma in probability density function of lognorm.
from scipy.stats import lognorm
stddev = 0.859455801705594
mu = 0.418749176686875
total = 37
dist = lognorm.cdf(total,mu,stddev)
UPDATE:
So after a bit of work and a little research, I got a little further. But I still am getting the wrong answer. The new code is below. According to R and Excel, the result should be .7434, but that's clearly not what is happening. Is there a logic flaw I am missing?
stddev = 2.0785
mu = 1.744
x = 25
dist = lognorm([mu],loc=stddev)
dist.cdf(x)  # yields=0.96374596, expected=0.7434
A:
<code>
import numpy as np
from scipy import stats
stddev = 2.0785
mu = 1.744
x = 25
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>

AnswerGT: <NO_SOLUTION>
MostLikelyAns: result = np.exp(stats.lognorm.logcdf(x, mu, stddev))


 
 

Prompt: Problem:

Here is some code example. To better understand it, I'm trying to train models with GradientBoostingClassifier with categorical variables as input.

from sklearn import datasets
from sklearn.ensemble import GradientBoostingClassifier
import pandas

iris = datasets.load_iris()
X = iris.data[(iris.target==0) | (iris.target==1)]
Y = iris.target[(iris.target==0) | (iris.target==1)]
train_indices = list(range(40)) + list(range(50,90))
test_indices = list(range(40,50)) + list(range(90,100))
X_train = X[train_indices]
X_test = X[test_indices]
y_train = Y[train_indices]
y_test = Y[test_indices]
X_train = pandas.DataFrame(X_train)
X_train[0] = ['a']*40 + ['b']*40
clf = GradientBoostingClassifier(learning_rate=0.01,max_depth=8,n_estimators=50).fit(X_train, y_train)

This piece of code report error like:
ValueError: could not convert string to float: 'b'
I find it seems that One Hot Encoding on categorical variables is required before GradientBoostingClassifier.
But can GradientBoostingClassifier build models using categorical variables without one hot encoding? I want to convert categorical variable to matrix and merge back with original training data use get_dummies in pandas.
Could you give me some help how to use this function to handle this?

A:

<code>
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.ensemble import GradientBoostingClassifier
import pandas

# load data in the example
X_train, y_train = load_data()
X_train[0] = ['a'] * 40 + ['b'] * 40

</code>
X_train = ... # put solution in this variable
BEGIN SOLUTION
<code>

Problem: Problem:

Here is some code example. To better understand it, I'm trying to train models with GradientBoostingClassifier with categorical variables as input.

from sklearn import datasets
from sklearn.ensemble import GradientBoostingClassifier
import pandas

iris = datasets.load_iris()
X = iris.data[(iris.target==0) | (iris.target==1)]
Y = iris.target[(iris.target==0) | (iris.target==1)]
train_indices = list(range(40)) + list(range(50,90))
test_indices = list(range(40,50)) + list(range(90,100))
X_train = X[train_indices]
X_test = X[test_indices]
y_train = Y[train_indices]
y_test = Y[test_indices]
X_train = pandas.DataFrame(X_train)
X_train[0] = ['a']*40 + ['b']*40
clf = GradientBoostingClassifier(learning_rate=0.01,max_depth=8,n_estimators=50).fit(X_train, y_train)

This piece of code report error like:
ValueError: could not convert string to float: 'b'
I find it seems that One Hot Encoding on categorical variables is required before GradientBoostingClassifier.
But can GradientBoostingClassifier build models using categorical variables without one hot encoding? I want to convert categorical variable to matrix and merge back with original training data use get_dummies in pandas.
Could you give me some help how to use this function to handle this?

A:

<code>
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.ensemble import GradientBoostingClassifier
import pandas

# load data in the example
X_train, y_train = load_data()
X_train[0] = ['a'] * 40 + ['b'] * 40

</code>
X_train = ... # put solution in this variable
BEGIN SOLUTION
<code>

AnswerGT: <NO_SOLUTION>
MostLikelyAns: # load data in the example
X_train, y_train = load_data()

# one hot encoding
X_train = pd.get_dummies(X_train, columns=['column_name'])



 
 

Prompt: import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

x = np.arange(10)
y = np.arange(10)

# Plot y over x in a line chart and label the line "y over x"
# Show legend of the plot and give the legend box a title  "Legend"
# Bold the legend title
# SOLUTION START

Problem: import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

x = np.arange(10)
y = np.arange(10)

# Plot y over x in a line chart and label the line "y over x"
# Show legend of the plot and give the legend box a title  "Legend"
# Bold the legend title
# SOLUTION START

AnswerGT: <NO_SOLUTION>
MostLikelyAns: plt.plot(x, y, label="y over x")
plt.legend(["y over x"], loc="upper right", title="Legend", fontsize=8)
plt.xlabel("x")
plt.ylabel("y")
plt.title("Line Plot")


 
 

Prompt: Problem:
I have following pandas dataframe :


import pandas as pd
from pandas import Series, DataFrame
data = DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],
              'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],
              'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})


I'd like to change values in columns Qu1 according to value_counts() when value count great or equal 3 and change values in columns Qu2 and Qu3 according to value_counts() when value count great or equal 2.
For example for Qu1 column
>>> pd.value_counts(data.Qu1) >= 3
cheese     True
potato    False
banana    False
apple     False
egg       False


I'd like to keep values cheese, because each value has at least three appearances.
From values potato, banana, apple and egg I'd like to create value others
For column Qu2 no changes :
>>> pd.value_counts(data.Qu2) >= 2
banana     True
apple      True
sausage   True


The final result as in attached test_data
test_data = DataFrame({'Qu1': ['other', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],
                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],
                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']})


Thanks !




A:
<code>
import pandas as pd


df = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],
                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],
                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>

Problem: Problem:
I have following pandas dataframe :


import pandas as pd
from pandas import Series, DataFrame
data = DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],
              'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],
              'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})


I'd like to change values in columns Qu1 according to value_counts() when value count great or equal 3 and change values in columns Qu2 and Qu3 according to value_counts() when value count great or equal 2.
For example for Qu1 column
>>> pd.value_counts(data.Qu1) >= 3
cheese     True
potato    False
banana    False
apple     False
egg       False


I'd like to keep values cheese, because each value has at least three appearances.
From values potato, banana, apple and egg I'd like to create value others
For column Qu2 no changes :
>>> pd.value_counts(data.Qu2) >= 2
banana     True
apple      True
sausage   True


The final result as in attached test_data
test_data = DataFrame({'Qu1': ['other', 'other', 'cheese', 'other', 'cheese', 'other', 'cheese', 'other', 'other'],
                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],
                  'Qu3': ['other', 'potato', 'other', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'other']})


Thanks !




A:
<code>
import pandas as pd


df = pd.DataFrame({'Qu1': ['apple', 'potato', 'cheese', 'banana', 'cheese', 'banana', 'cheese', 'potato', 'egg'],
                   'Qu2': ['sausage', 'banana', 'apple', 'apple', 'apple', 'sausage', 'banana', 'banana', 'banana'],
                   'Qu3': ['apple', 'potato', 'sausage', 'cheese', 'cheese', 'potato', 'cheese', 'potato', 'egg']})
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>

AnswerGT: <NO_SOLUTION>
MostLikelyAns: result = pd.DataFrame({'Qu1': [...],
                       'Qu2': [...],
                       'Qu3': [...]})


 
 

Prompt: Problem:
I have some data that comes in the form (x, y, z, V) where x,y,z are distances, and V is the moisture. I read a lot on StackOverflow about interpolation by python like this and this valuable posts, but all of them were about regular grids of x, y, z. i.e. every value of x contributes equally with every point of y, and every point of z. On the other hand, my points came from 3D finite element grid (as below), where the grid is not regular. 
The two mentioned posts 1 and 2, defined each of x, y, z as a separate numpy array then they used something like cartcoord = zip(x, y) then scipy.interpolate.LinearNDInterpolator(cartcoord, z) (in a 3D example). I can not do the same as my 3D grid is not regular, thus not each point has a contribution to other points, so if when I repeated these approaches I found many null values, and I got many errors.
Here are 10 sample points in the form of [x, y, z, V]
data = [[27.827, 18.530, -30.417, 0.205] , [24.002, 17.759, -24.782, 0.197] , 
[22.145, 13.687, -33.282, 0.204] , [17.627, 18.224, -25.197, 0.197] , 
[29.018, 18.841, -38.761, 0.212] , [24.834, 20.538, -33.012, 0.208] , 
[26.232, 22.327, -27.735, 0.204] , [23.017, 23.037, -29.230, 0.205] , 
[28.761, 21.565, -31.586, 0.211] , [26.263, 23.686, -32.766, 0.215]]

I want to get the interpolated value V of the point (25, 20, -30) and (27, 20, -32) as a list.
How can I get it?

A:
<code>
import numpy as np
import scipy.interpolate

points = np.array([
        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],
        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],
        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],
        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],
        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])
V = np.array([0.205,  0.197,  0.204,  0.197,  0.212,
                   0.208,  0.204,  0.205, 0.211,  0.215])
request = np.array([[25, 20, -30], [27, 20, -32]])
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>

Problem: Problem:
I have some data that comes in the form (x, y, z, V) where x,y,z are distances, and V is the moisture. I read a lot on StackOverflow about interpolation by python like this and this valuable posts, but all of them were about regular grids of x, y, z. i.e. every value of x contributes equally with every point of y, and every point of z. On the other hand, my points came from 3D finite element grid (as below), where the grid is not regular. 
The two mentioned posts 1 and 2, defined each of x, y, z as a separate numpy array then they used something like cartcoord = zip(x, y) then scipy.interpolate.LinearNDInterpolator(cartcoord, z) (in a 3D example). I can not do the same as my 3D grid is not regular, thus not each point has a contribution to other points, so if when I repeated these approaches I found many null values, and I got many errors.
Here are 10 sample points in the form of [x, y, z, V]
data = [[27.827, 18.530, -30.417, 0.205] , [24.002, 17.759, -24.782, 0.197] , 
[22.145, 13.687, -33.282, 0.204] , [17.627, 18.224, -25.197, 0.197] , 
[29.018, 18.841, -38.761, 0.212] , [24.834, 20.538, -33.012, 0.208] , 
[26.232, 22.327, -27.735, 0.204] , [23.017, 23.037, -29.230, 0.205] , 
[28.761, 21.565, -31.586, 0.211] , [26.263, 23.686, -32.766, 0.215]]

I want to get the interpolated value V of the point (25, 20, -30) and (27, 20, -32) as a list.
How can I get it?

A:
<code>
import numpy as np
import scipy.interpolate

points = np.array([
        [ 27.827,  18.53 , -30.417], [ 24.002,  17.759, -24.782],
        [ 22.145,  13.687, -33.282], [ 17.627,  18.224, -25.197],
        [ 29.018,  18.841, -38.761], [ 24.834,  20.538, -33.012],
        [ 26.232,  22.327, -27.735], [ 23.017,  23.037, -29.23 ],
        [ 28.761,  21.565, -31.586], [ 26.263,  23.686, -32.766]])
V = np.array([0.205,  0.197,  0.204,  0.197,  0.212,
                   0.208,  0.204,  0.205, 0.211,  0.215])
request = np.array([[25, 20, -30], [27, 20, -32]])
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>

AnswerGT: <NO_SOLUTION>
MostLikelyAns: result = scipy.interpolate.interp2d(points[:, 0], points[:, 1], V, kind='cubic')(request[:, 0], request[:, 1])


 
 

Prompt: Problem:

Is there any package in Python that does data transformation like Yeo-Johnson transformation to eliminate skewness of data?
I know about sklearn, but I was unable to find functions to do Yeo-Johnson transformation.
How can I use sklearn to solve this?

A:

<code>
import numpy as np
import pandas as pd
import sklearn
data = load_data()
assert type(data) == np.ndarray
</code>
yeo_johnson_data = ... # put solution in this variable
BEGIN SOLUTION
<code>

Problem: Problem:

Is there any package in Python that does data transformation like Yeo-Johnson transformation to eliminate skewness of data?
I know about sklearn, but I was unable to find functions to do Yeo-Johnson transformation.
How can I use sklearn to solve this?

A:

<code>
import numpy as np
import pandas as pd
import sklearn
data = load_data()
assert type(data) == np.ndarray
</code>
yeo_johnson_data = ... # put solution in this variable
BEGIN SOLUTION
<code>

AnswerGT: <NO_SOLUTION>
MostLikelyAns: 
from scipy.stats import skewnorm
skewnorm.rvs(a=0, loc=0, scale=1, size=None)


 
 

Prompt: import matplotlib.pyplot as plt

# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]
# SOLUTION START

Problem: import matplotlib.pyplot as plt

# draw vertical lines at [0.22058956, 0.33088437, 2.20589566]
# SOLUTION START

AnswerGT: <NO_SOLUTION>
MostLikelyAns: def vertical_lines(ax, x, y):
    for x_i, y_i in zip(x, y):
        ax.axvline(x_i, color='red')
        ax.axhline(y_i, color='red')



 
 

Prompt: Problem:
My sample df has four columns with NaN values. The goal is to concatenate all the rows while excluding the NaN values. 
import pandas as pd
import numpy as np
df = pd.DataFrame({'keywords_0':["a", np.nan, "c"], 
                'keywords_1':["d", "e", np.nan],
                'keywords_2':[np.nan, np.nan, "b"],
                'keywords_3':["f", np.nan, "g"]})
  keywords_0 keywords_1 keywords_2 keywords_3
0          a          d        NaN          f
1        NaN          e        NaN        NaN
2          c        NaN          b          g


Want to accomplish the following:
  keywords_0 keywords_1 keywords_2 keywords_3 keywords_all
0          a          d        NaN          f        a-d-f
1        NaN          e        NaN        NaN            e
2          c        NaN          b          g        c-b-g


Pseudo code:
cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]
df["keywords_all"] = df["keywords_all"].apply(lambda cols: "-".join(cols), axis=1)


I know I can use "-".join() to get the exact result, but I am unsure how to pass the column names into the function.


A:
<code>
import pandas as pd
import numpy as np


df = pd.DataFrame({'keywords_0':["a", np.nan, "c"], 
                'keywords_1':["d", "e", np.nan],
                'keywords_2':[np.nan, np.nan, "b"],
                'keywords_3':["f", np.nan, "g"]})
</code>
df = ... # put solution in this variable
BEGIN SOLUTION
<code>

Problem: Problem:
My sample df has four columns with NaN values. The goal is to concatenate all the rows while excluding the NaN values. 
import pandas as pd
import numpy as np
df = pd.DataFrame({'keywords_0':["a", np.nan, "c"], 
                'keywords_1':["d", "e", np.nan],
                'keywords_2':[np.nan, np.nan, "b"],
                'keywords_3':["f", np.nan, "g"]})
  keywords_0 keywords_1 keywords_2 keywords_3
0          a          d        NaN          f
1        NaN          e        NaN        NaN
2          c        NaN          b          g


Want to accomplish the following:
  keywords_0 keywords_1 keywords_2 keywords_3 keywords_all
0          a          d        NaN          f        a-d-f
1        NaN          e        NaN        NaN            e
2          c        NaN          b          g        c-b-g


Pseudo code:
cols = [df.keywords_0, df.keywords_1, df.keywords_2, df.keywords_3]
df["keywords_all"] = df["keywords_all"].apply(lambda cols: "-".join(cols), axis=1)


I know I can use "-".join() to get the exact result, but I am unsure how to pass the column names into the function.


A:
<code>
import pandas as pd
import numpy as np


df = pd.DataFrame({'keywords_0':["a", np.nan, "c"], 
                'keywords_1':["d", "e", np.nan],
                'keywords_2':[np.nan, np.nan, "b"],
                'keywords_3':["f", np.nan, "g"]})
</code>
df = ... # put solution in this variable
BEGIN SOLUTION
<code>

AnswerGT: <NO_SOLUTION>
MostLikelyAns: df["keywords_all"] = "-".join(df[["keywords_0", "keywords_1", "keywords_2", "keywords_3"]].values.ravel())


 
 

Prompt: Problem:
I have many duplicate records - some of them have a bank account. I want to keep the records with a bank account. 
Basically something like:
if there are two Tommy Joes:
     keep the one with a bank account


I have tried to dedupe with the code below, but it is keeping the dupe with no bank account. 
df = pd.DataFrame({'firstname':['foo Bar','Bar Bar','Foo Bar','jim','john','mary','jim'],
                   'lastname':['Foo Bar','Bar','Foo Bar','ryan','con','sullivan','Ryan'],
                   'email':['Foo bar','Bar','Foo Bar','jim@com','john@com','mary@com','Jim@com'],
                   'bank':[np.nan,'abc','xyz',np.nan,'tge','vbc','dfg']})
df
  firstname  lastname     email bank
0   foo Bar   Foo Bar   Foo bar  NaN  
1   Bar Bar       Bar       Bar  abc
2   Foo Bar   Foo Bar   Foo Bar  xyz
3       jim      ryan   jim@com  NaN
4      john       con  john@com  tge
5      mary  sullivan  mary@com  vbc
6       jim      Ryan   Jim@com  dfg
# get the index of unique values, based on firstname, lastname, email
# convert to lower and remove white space first
uniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])
.applymap(lambda s:s.lower() if type(s) == str else s)
.applymap(lambda x: x.replace(" ", "") if type(x)==str else x)
.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index
# save unique records
dfiban_uniq = df.loc[uniq_indx]
dfiban_uniq
  firstname  lastname     email bank
0   foo Bar   Foo Bar   Foo bar  NaN # should not be here
1   Bar Bar       Bar       Bar  abc
3       jim      ryan   jim@com  NaN # should not be here
4      john       con  john@com  tge
5      mary  sullivan  mary@com  vbc
# I wanted these duplicates to appear in the result:
  firstname  lastname     email bank
2   Foo Bar   Foo Bar   Foo Bar  xyz  
6       jim      Ryan   Jim@com  dfg


You can see index 0 and 3 were kept. The versions of these customers with bank accounts were removed. My expected result is to have it the other way around. Remove the dupes that don't have an bank account. 
I have thought about doing a sort by bank account first, but I have so much data, I am unsure how to 'sense check' it to see if it works. 
Any help appreciated. 
There are a few similar questions here but all of them seem to have values that can be sorted such as age etc. These hashed bank account numbers are very messy

A:
<code>
import pandas as pd
import numpy as np


df = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar'],
                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar'],
                   'email': ['Foo bar', 'Bar', 'Foo Bar'],
                   'bank': [np.nan, 'abc', 'xyz']})
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>

Problem: Problem:
I have many duplicate records - some of them have a bank account. I want to keep the records with a bank account. 
Basically something like:
if there are two Tommy Joes:
     keep the one with a bank account


I have tried to dedupe with the code below, but it is keeping the dupe with no bank account. 
df = pd.DataFrame({'firstname':['foo Bar','Bar Bar','Foo Bar','jim','john','mary','jim'],
                   'lastname':['Foo Bar','Bar','Foo Bar','ryan','con','sullivan','Ryan'],
                   'email':['Foo bar','Bar','Foo Bar','jim@com','john@com','mary@com','Jim@com'],
                   'bank':[np.nan,'abc','xyz',np.nan,'tge','vbc','dfg']})
df
  firstname  lastname     email bank
0   foo Bar   Foo Bar   Foo bar  NaN  
1   Bar Bar       Bar       Bar  abc
2   Foo Bar   Foo Bar   Foo Bar  xyz
3       jim      ryan   jim@com  NaN
4      john       con  john@com  tge
5      mary  sullivan  mary@com  vbc
6       jim      Ryan   Jim@com  dfg
# get the index of unique values, based on firstname, lastname, email
# convert to lower and remove white space first
uniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])
.applymap(lambda s:s.lower() if type(s) == str else s)
.applymap(lambda x: x.replace(" ", "") if type(x)==str else x)
.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index
# save unique records
dfiban_uniq = df.loc[uniq_indx]
dfiban_uniq
  firstname  lastname     email bank
0   foo Bar   Foo Bar   Foo bar  NaN # should not be here
1   Bar Bar       Bar       Bar  abc
3       jim      ryan   jim@com  NaN # should not be here
4      john       con  john@com  tge
5      mary  sullivan  mary@com  vbc
# I wanted these duplicates to appear in the result:
  firstname  lastname     email bank
2   Foo Bar   Foo Bar   Foo Bar  xyz  
6       jim      Ryan   Jim@com  dfg


You can see index 0 and 3 were kept. The versions of these customers with bank accounts were removed. My expected result is to have it the other way around. Remove the dupes that don't have an bank account. 
I have thought about doing a sort by bank account first, but I have so much data, I am unsure how to 'sense check' it to see if it works. 
Any help appreciated. 
There are a few similar questions here but all of them seem to have values that can be sorted such as age etc. These hashed bank account numbers are very messy

A:
<code>
import pandas as pd
import numpy as np


df = pd.DataFrame({'firstname': ['foo Bar', 'Bar Bar', 'Foo Bar'],
                   'lastname': ['Foo Bar', 'Bar', 'Foo Bar'],
                   'email': ['Foo bar', 'Bar', 'Foo Bar'],
                   'bank': [np.nan, 'abc', 'xyz']})
</code>
result = ... # put solution in this variable
BEGIN SOLUTION
<code>

AnswerGT: <NO_SOLUTION>
MostLikelyAns: # get the index of unique values, based on firstname, lastname, email
# convert to lower and remove white space first
uniq_indx = (df.dropna(subset=['firstname', 'lastname', 'email'])
.applymap(lambda s:s.lower() if type(s) == str else s)
.applymap(lambda x: x.replace(" ", "") if type(x)==str else x)
.drop_duplicates(subset=['firstname', 'lastname', 'email'], keep='first')).index

# save unique records
dfiban_uniq = df.loc[uniq_indx]
print(dfiban_uniq)



 
 

Prompt: import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = {
    "reports": [4, 24, 31, 2, 3],
    "coverage": [35050800, 54899767, 57890789, 62890798, 70897871],
}
df = pd.DataFrame(data)
sns.catplot(y="coverage", x="reports", kind="bar", data=df, label="Total")

# do not use scientific notation in the y axis ticks labels
# SOLUTION START

Problem: import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = {
    "reports": [4, 24, 31, 2, 3],
    "coverage": [35050800, 54899767, 57890789, 62890798, 70897871],
}
df = pd.DataFrame(data)
sns.catplot(y="coverage", x="reports", kind="bar", data=df, label="Total")

# do not use scientific notation in the y axis ticks labels
# SOLUTION START

AnswerGT: <NO_SOLUTION>
MostLikelyAns: plt.ticklabel_format(useOffset=False)


 
 

